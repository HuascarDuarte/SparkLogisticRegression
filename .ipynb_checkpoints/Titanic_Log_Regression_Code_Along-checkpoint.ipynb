{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Code Along\n",
    "This is a code along of the famous titanic dataset, its always nice to start off with this dataset because it is an example you will find across pretty much every data analysis language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/huascar/spark-3.5.6-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/19 12:27:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('myproj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('titanic.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------+\n",
      "|summary|      PassengerId|            Pclass|               Age|   Sex|\n",
      "+-------+-----------------+------------------+------------------+------+\n",
      "|  count|              891|               891|               714|   891|\n",
      "|   mean|            446.0| 2.308641975308642| 29.69911764705882|  NULL|\n",
      "| stddev|257.3538420152301|0.8360712409770491|14.526497332334035|  NULL|\n",
      "|    min|                1|                 1|              0.42|female|\n",
      "|    max|              891|                 3|              80.0|  male|\n",
      "+-------+-----------------+------------------+------------------+------+\n",
      "\n",
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| NULL|    NULL|\n",
      "| stddev|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| NULL|    NULL|\n",
      "|    min|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['PassengerId','Pclass','Age', 'Sex'].describe().show()\n",
    "data['SibSp', 'Parch', 'Ticket','Fare','Cabin', 'Embarked'].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|  0|    0|    0|     0|   0|    0|       0|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find number of nulls in each column\n",
    "from pyspark.sql import functions as F\n",
    "data.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Embarked|\n",
      "+--------+\n",
      "|       Q|\n",
      "|       C|\n",
      "|       S|\n",
      "|    NULL|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"Embarked\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Embarked|\n",
      "+--------+\n",
      "|       Q|\n",
      "|       C|\n",
      "|       X|\n",
      "|       S|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Treat NULLS in Embarked\n",
    "data = data.fillna({\"Embarked\": \"X\"})\n",
    "data.select(\"Embarked\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treat NULLS in Age: replace with mean value = 29.7\n",
    "data = data.fillna({\"Age\": 29.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove cabin column\n",
    "data = data['Survived',\n",
    " 'Pclass',\n",
    " 'Name',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Ticket',\n",
    " 'Fare',\n",
    "    'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bypass nulls treatment removing rows with nulls\n",
    "data = data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+---+---+-----+-----+------+----+--------+\n",
      "|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Embarked|\n",
      "+--------+------+----+---+---+-----+-----+------+----+--------+\n",
      "|       0|     0|   0|  0|  0|    0|    0|     0|   0|       0|\n",
      "+--------+------+----+---+---+-----+-----+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------+\n",
      "|summary|            Pclass|              Age|   Sex|\n",
      "+-------+------------------+-----------------+------+\n",
      "|  count|               712|              712|   712|\n",
      "|   mean| 2.240168539325843|29.64209269662921|  NULL|\n",
      "| stddev|0.8368543166903446|14.49293290032352|  NULL|\n",
      "|    min|                 1|             0.42|female|\n",
      "|    max|                 3|             80.0|  male|\n",
      "+-------+------------------+-----------------+------+\n",
      "\n",
      "+-------+------------------+-------------------+-----------------+------------------+--------+\n",
      "|summary|             SibSp|              Parch|           Ticket|              Fare|Embarked|\n",
      "+-------+------------------+-------------------+-----------------+------------------+--------+\n",
      "|  count|               712|                712|              712|               712|     712|\n",
      "|   mean|0.5140449438202247|0.43258426966292135|276349.1541425819| 34.56725140449432|    NULL|\n",
      "| stddev|0.9306921267673427| 0.8541814457454133|524618.6556654529|52.938648174710906|    NULL|\n",
      "|    min|                 0|                  0|           110152|               0.0|       C|\n",
      "|    max|                 5|                  6|        WE/P 5735|          512.3292|       S|\n",
      "+-------+------------------+-------------------+-----------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data['Pclass','Age', 'Sex'].describe().show()\n",
    "data['SibSp', 'Parch', 'Ticket','Fare', 'Embarked'].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------------+--------+\n",
      "|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Embarked|EmbarkedIndex|SexIndex|\n",
      "+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------------+--------+\n",
      "|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       S|          0.0|     0.0|\n",
      "|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|       C|          1.0|     1.0|\n",
      "|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       S|          0.0|     1.0|\n",
      "|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       S|          0.0|     1.0|\n",
      "|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       S|          0.0|     0.0|\n",
      "|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|       S|          0.0|     0.0|\n",
      "|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075|       S|          0.0|     0.0|\n",
      "|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       S|          0.0|     1.0|\n",
      "|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       C|          1.0|     1.0|\n",
      "|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|       S|          0.0|     1.0|\n",
      "|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       S|          0.0|     1.0|\n",
      "|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       S|          0.0|     0.0|\n",
      "|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       S|          0.0|     0.0|\n",
      "|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       S|          0.0|     1.0|\n",
      "|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       S|          0.0|     1.0|\n",
      "|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125|       Q|          2.0|     0.0|\n",
      "|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       S|          0.0|     1.0|\n",
      "|       0|     2|Fynney, Mr. Joseph J|  male|35.0|    0|    0|          239865|   26.0|       S|          0.0|     0.0|\n",
      "|       1|     2|Beesley, Mr. Lawr...|  male|34.0|    0|    0|          248698|   13.0|       S|          0.0|     0.0|\n",
      "|       1|     3|\"McGowan, Miss. A...|female|15.0|    0|    0|          330923| 8.0292|       Q|          2.0|     1.0|\n",
      "+--------+------+--------------------+------+----+-----+-----+----------------+-------+--------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+-------------+\n",
      "|Embarked|EmbarkedIndex|\n",
      "+--------+-------------+\n",
      "|       Q|          2.0|\n",
      "|       S|          0.0|\n",
      "|       C|          1.0|\n",
      "+--------+-------------+\n",
      "\n",
      "+------+--------+\n",
      "|   Sex|SexIndex|\n",
      "+------+--------+\n",
      "|  male|     0.0|\n",
      "|female|     1.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create indexer for Embarked and Sex\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "embarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "data = embarkedIndexer.fit(data).transform(data)\n",
    "sexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "data = sexIndexer.fit(data).transform(data)\n",
    "data.show()\n",
    "data.select(\"Embarked\", \"EmbarkedIndex\").distinct().show()\n",
    "data.select(\"Sex\", \"SexIndex\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked',\n",
       " 'EmbarkedIndex',\n",
       " 'SexIndex']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+----+-------------+\n",
      "|Survived|Pclass|SexIndex| Age|EmbarkedIndex|\n",
      "+--------+------+--------+----+-------------+\n",
      "|       0|     3|     0.0|22.0|          0.0|\n",
      "|       1|     1|     1.0|38.0|          1.0|\n",
      "|       1|     3|     1.0|26.0|          0.0|\n",
      "|       1|     1|     1.0|35.0|          0.0|\n",
      "|       0|     3|     0.0|35.0|          0.0|\n",
      "|       0|     1|     0.0|54.0|          0.0|\n",
      "|       0|     3|     0.0| 2.0|          0.0|\n",
      "|       1|     3|     1.0|27.0|          0.0|\n",
      "|       1|     2|     1.0|14.0|          1.0|\n",
      "|       1|     3|     1.0| 4.0|          0.0|\n",
      "|       1|     1|     1.0|58.0|          0.0|\n",
      "|       0|     3|     0.0|20.0|          0.0|\n",
      "|       0|     3|     0.0|39.0|          0.0|\n",
      "|       0|     3|     1.0|14.0|          0.0|\n",
      "|       1|     2|     1.0|55.0|          0.0|\n",
      "|       0|     3|     0.0| 2.0|          2.0|\n",
      "|       0|     3|     1.0|31.0|          0.0|\n",
      "|       0|     2|     0.0|35.0|          0.0|\n",
      "|       1|     2|     0.0|34.0|          0.0|\n",
      "|       1|     3|     1.0|15.0|          2.0|\n",
      "+--------+------+--------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data = data.select(['Survived',\n",
    " 'Pclass',\n",
    " 'SexIndex',\n",
    " 'Age',\n",
    " #'SibSp',\n",
    " #'Parch',\n",
    " #'Fare',\n",
    " 'EmbarkedIndex'])\n",
    "my_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Categorical Columns\n",
    "\n",
    "Let's break this down into multiple steps to make it all clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexVec',\n",
    " 'Age',\n",
    " #'SibSp',\n",
    " #'Parch',\n",
    " #'Fare',\n",
    " 'EmbarkVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+----+-------------+------------------+\n",
      "|Survived|Pclass|SexIndex| Age|EmbarkedIndex|          features|\n",
      "+--------+------+--------+----+-------------+------------------+\n",
      "|       0|     3|     0.0|22.0|          0.0|[3.0,0.0,22.0,0.0]|\n",
      "|       1|     1|     1.0|38.0|          1.0|[1.0,1.0,38.0,1.0]|\n",
      "|       1|     3|     1.0|26.0|          0.0|[3.0,1.0,26.0,0.0]|\n",
      "|       1|     1|     1.0|35.0|          0.0|[1.0,1.0,35.0,0.0]|\n",
      "|       0|     3|     0.0|35.0|          0.0|[3.0,0.0,35.0,0.0]|\n",
      "|       0|     1|     0.0|54.0|          0.0|[1.0,0.0,54.0,0.0]|\n",
      "|       0|     3|     0.0| 2.0|          0.0| [3.0,0.0,2.0,0.0]|\n",
      "|       1|     3|     1.0|27.0|          0.0|[3.0,1.0,27.0,0.0]|\n",
      "|       1|     2|     1.0|14.0|          1.0|[2.0,1.0,14.0,1.0]|\n",
      "|       1|     3|     1.0| 4.0|          0.0| [3.0,1.0,4.0,0.0]|\n",
      "|       1|     1|     1.0|58.0|          0.0|[1.0,1.0,58.0,0.0]|\n",
      "|       0|     3|     0.0|20.0|          0.0|[3.0,0.0,20.0,0.0]|\n",
      "|       0|     3|     0.0|39.0|          0.0|[3.0,0.0,39.0,0.0]|\n",
      "|       0|     3|     1.0|14.0|          0.0|[3.0,1.0,14.0,0.0]|\n",
      "|       1|     2|     1.0|55.0|          0.0|[2.0,1.0,55.0,0.0]|\n",
      "|       0|     3|     0.0| 2.0|          2.0| [3.0,0.0,2.0,2.0]|\n",
      "|       0|     3|     1.0|31.0|          0.0|[3.0,1.0,31.0,0.0]|\n",
      "|       0|     2|     0.0|35.0|          0.0|[2.0,0.0,35.0,0.0]|\n",
      "|       1|     2|     0.0|34.0|          0.0|[2.0,0.0,34.0,0.0]|\n",
      "|       1|     3|     1.0|15.0|          2.0|[3.0,1.0,15.0,2.0]|\n",
      "+--------+------+--------+----+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexIndex',\n",
    " 'Age',\n",
    " #'SibSp',\n",
    " #'Parch',\n",
    " #'Fare',\n",
    " 'EmbarkedIndex'],outputCol='features')\n",
    "df = assembler.transform(my_data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([3.0, 0.0, 22.0, 1.0, 0.0, 7.25, 0.0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.PassengerId.isin([1])).select('features').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=SparseVector(7, {0: 3.0, 2: 35.0, 5: 8.05}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.PassengerId.isin([5])).select('features').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRec = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRecModel = logRec.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = logRecModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results['Survived','prediction'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7589995324918185"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC = evaluation.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "Let's see an example of how to use pipelines (we'll get a lot more practice with these later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_titanic_data, test_titanic_data = my_final_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7918269230769232"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
